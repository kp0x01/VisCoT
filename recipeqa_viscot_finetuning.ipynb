{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "YjbZa_fPFBj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUHWrXb4E3HK",
        "outputId": "2d044888-1996-4d6e-b3d1-2ea47a181230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗️need to make sure python version is less than 3.12"
      ],
      "metadata": {
        "id": "I5sI0diPc3Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSb2Qz5ISEBS",
        "outputId": "fda222eb-9a60-45ff-c4b5-eacc7f2f783f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.31.0 peft==0.4.0 accelerate==0.20.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L4Zbl2ETRe4L",
        "outputId": "147dbae7-7f14-4d53-a0b1-8a2997f76754"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.4.0\n",
            "  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting accelerate==0.20.3\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.4.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.4.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft==0.4.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.4.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.4.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (3.0.2)\n",
            "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, transformers, nvidia-cusolver-cu12, accelerate, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.2\n",
            "    Uninstalling transformers-4.53.2:\n",
            "      Successfully uninstalled transformers-4.53.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.8.1\n",
            "    Uninstalling accelerate-1.8.1:\n",
            "      Successfully uninstalled accelerate-1.8.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.16.0\n",
            "    Uninstalling peft-0.16.0:\n",
            "      Successfully uninstalled peft-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.20.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.4.0 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wHfVpLCudOIr",
        "outputId": "b99b8c99-7ad7-43de-833a-726392952be5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.18.2.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.1)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.2-py3-none-any.whl size=1763311 sha256=47fc124b9b483d17dbcf2c98d074d3262fd00485389bc5e0b29a95e972eb5947\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/2b/fe/f2625302f25976b9828a27d3d2567bdc1d587a1ff0ab42c8a9\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.18.2 hjson-3.1.0 ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Setup"
      ],
      "metadata": {
        "id": "GWejz4bVg39y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/GenAI\\ Project/VisCoT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POdd3mnsJJ7n",
        "outputId": "bbe0f249-da34-473e-921d-9bace22dcfc6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GenAI Project/VisCoT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from PIL import Image\n",
        "\n",
        "from llava.train.train import train\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS3nUtG_ibTB",
        "outputId": "4930b70c-0d70-47e3-9042-d8c50bf01522"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.6.0+cu124\n",
            "CUDA Available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download models"
      ],
      "metadata": {
        "id": "Z8EQoZDsKhCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "LLAVA_MODEL = \"liuhaotian/llava-v1.5-7b\"\n",
        "VISION_TOWER = \"openai/clip-vit-large-patch14-336\"\n",
        "\n",
        "print(\"Downloading models...\\n\")\n",
        "\n",
        "print(f\"1. {LLAVA_MODEL}\")\n",
        "llava_path = snapshot_download(repo_id=LLAVA_MODEL)\n",
        "print(f\"   Downloaded to: {llava_path}\\n\")\n",
        "\n",
        "print(f\"2. {VISION_TOWER}\")\n",
        "clip_path = snapshot_download(repo_id=VISION_TOWER)\n",
        "print(f\"   Downloaded to: {clip_path}\\n\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "44ab6796f4924a99b3549d448a74f2aa",
            "87945efd3c8c4bf6b4daa4f52490c63c",
            "51a561dc12474b06bb26071b17548114",
            "ce3cda07f07640d0955a6b2161d8ee03",
            "75dd908761504919a9f92836ab0d193d",
            "bf0d4cc52c064a5abb62591a9fa8608a",
            "5074d8016785418fa1762ce713568418",
            "ba4de516ead34276a39ec5295bca70a1",
            "d24a79ed6b0a4795b998ac66fcfa61bf",
            "a92b2cc8ef654971ba627529a1c441c0",
            "4de03cb7d14a4e73a18dace1e63b5c60",
            "8fb03ff4604a4b19baa2e5939fa473f6",
            "d0a277a5c06643bc8453656114abfc32",
            "a07e27a8457c4822a205a0b53d982a7a",
            "3c985979ae5e444298b35713b2b3168e",
            "b66e4cb2bcb04bac92f3f0daac62c874",
            "9b36c3ad8a624cbaaaeb3fe362a3ffbd",
            "33ef371415e74f46b0d2f9f4e5070b01",
            "78c5d113eb424bac85a115d4616132c3",
            "2539f398edea42ff9eafe965094db89d",
            "d0991fa6184e42ea92ee2cac49ec765b",
            "04662629594e4a8fa3532fea19034d0d",
            "e1ec3d3a0b0f4a17aec32de94cf53da7",
            "09256e5e4fba434db526c34dbe24344c",
            "226c56d976844fe0899b4d7eb57e3f88",
            "02e8674f3bca44b88b7fb193e93ad047",
            "0059b2d3893d4ecb868cedf373564ef2",
            "93c42022322d41f882670aca37635bda",
            "c7a69d5a4f2b4707917ecf825d91d559",
            "9d3d24981a024c10b4933e10a6be9db6",
            "b442d3f37ab1464ebf3bb12f57dd24cb",
            "31185b1aa7704fbeb05db6e6b016ddb2",
            "feaf852389e44c1a9a1a89639be32dc2",
            "489e967075164ed5b761b5ed1f25f805",
            "d562f2bb61dd4f07b10dfbcb9e8e2986",
            "1b13d442b5a4454fb3223f2c1c7c6d9c",
            "aa821aec40c74da8b7c38af31ed5e9d4",
            "7733d9ae665340d991b21add4bc71fa4",
            "0c843f4a620b47868dcd0757800cc4d2",
            "a0ec8bbfe2df4f89b5bf8627e2bb4035",
            "adde2cb4e2ba4a37a3503e3b3632e3ca",
            "eaef238b5eb0446c998f8e238feb9fc5",
            "06df68fc744c410ca11bb2b42c4864d2",
            "8ab874c8517a467baf2fea9cb692e47b",
            "7d6a3b8678a341f3b4306eadb7728f3e",
            "c5dba46a7b704a179d51fc27eb9e186b",
            "7b84e8cc72c9450a9f4553ce55e37c62",
            "1a53fb86ccf645dd8df75a6f9b68ff55",
            "045e2c42c4024be8bcb1ca9e5626d1cf",
            "495674b0c4924b92b47961519bec6962",
            "269cdbc5a72e46a78f13db1d0520b84e",
            "80fa649fe6704423ae1fc902d836e1cc",
            "09eec5a5f70c4a0f8a620df66039ca3c",
            "1489febe5e0b4c3ea9566af7dc147fc4",
            "4088fabb6ef24b6986cc49da426c1634",
            "35894742414f4a9a958d53407590cce6",
            "ab2e3a62ab79403291e2f95e0877f0da",
            "a643bea23d044576afcbedb1ab214309",
            "b5cdcbdd7297480c93b9575684a8ee5e",
            "c0a28d59db934addab8587ec413d396f",
            "eed7ed531e77477b99cc0e0a43c3c9e5",
            "f2ba71afa11142a89fc64e30cb0a656e",
            "434f9727866c472c8d0772fb030b6139",
            "339a1d1d102642a198a9f56667e76ee4",
            "7ffc0b922aab4b3e8414353692142853",
            "0fc03a55c5fd44c3973e583564eb8e9e",
            "a2e4d9a812fa40dbae4d34207d56bb7b",
            "a99d8c06d5d241998489fa96982019e1",
            "0be05097c2274b3eb5e4c7a003d53777",
            "75e1d4709a0f4780865cc997bd2e89ee",
            "7ec95dbc048642d7bbfb8bea2b5d9587",
            "ed1ea3bdb8654b2e8f0627570914f36d",
            "437a1d92689e474db1e785226ba3dc91",
            "7b029fdd1f154c29b29b6a2da37a728c",
            "2f50bf4af5bb4cd0b4509f8462de44cc",
            "408874e4eed545239f6cc8715e467195",
            "27fd07ee1c82414fbc3ce14fdcc4b80b",
            "ace3d6f60bd8451c84156fb57a1e0fb5",
            "3cd14109ec07494692d1c5de58ab7d59",
            "0431170a855a4a8f8ffc04c18f68091a",
            "1a917fe0e6e14697a4ee73734420d1c5",
            "281322f0ef2348a098ffad8d40219c5e",
            "6d6d8209eb9a497788b2bd87fdf40108",
            "2166f85f640a4e9b92d57ab89808e360",
            "9e4c71e7e89a4f81a3d4fd100fc7de9a",
            "0ab3222d68a043378df2fdc71fcdaf6d",
            "4f97a1edd2d547a0a43b3c1a06743603",
            "2d1612617fbc4d1f9374bd1b5375e046",
            "13fcc7efea6644bd9b51341c2a022a8c",
            "c2bdadaee17d496698205a8bcb3fccbc",
            "eca0e671faba4a82853490d76d987501",
            "68ccdd7c8e214677aeb84169329a3a5c",
            "cb06dba60084465ea60174748c464c05",
            "d788d02f25bc4797b4f981a945d0cb09",
            "39a526a640dd471bb5435fc575efd42e",
            "05748e85bae341a3ba9bf819966de796",
            "cef8dc63fd714b0abb7bf7c0f4b998d6",
            "d7420794283e4fc4bf6e0aed8f76e45a",
            "37ba50d79ff24d459b16aa07e802a515",
            "f22b080ec5a94183a0ec33053e00f899",
            "92b9a1391f1f49639a012d22b79d04e8",
            "002ce2058ae547078bc069b68cb9add8",
            "6c590bc8b3914ffd9bf1773af8229408",
            "6a569b4c27624b499089f570250ebd0a",
            "e4695d87803e41099fc49f6ba4c2b4fd",
            "695ab972f10445daa36ea80136fac8d6",
            "cf716b6546a8431abc28f0175a1c8d88",
            "aad2650f6d6a4944ab847435171a1810",
            "0c087586df8f4c1da9ad4ef47e50083c",
            "0a870f1f39cb4026a6c8f82c9987f2e3",
            "c38dc9c43e484b0389c5d0ea2d260da5",
            "b9c91636727748898a1d66c0d053de95",
            "cf75fc7d932449bcbf8004476503c5b5",
            "444ee480349e41599fcf28750db523aa",
            "bd9869121c6d4e5d8642b0373ec30d6a",
            "f04d4d68ff3547fc9b420d73352bddaf",
            "4ba71669810b4330af0b9f452cb37190",
            "ffab0479098c4fd49b0f4642ec841aa0",
            "4d5e9d1203ad4b668be886f544c88318",
            "45628e46755b4b15bb46b43dbc879518",
            "e2eb43a7a3cc464aac5d6e283a2e149a",
            "5a8838f013274b3f9cbfcfe87ad82043",
            "d6c2139d544e4a8e9a7422fc50cf4354",
            "b0245d4a235046219b668a3c15322890",
            "537d64ac1ad3441286524e33d2b753dd",
            "e8f111b2ea9c4ab9ac37d5e55985b910",
            "c4e7aa99573147b39eb9c34b7e8eb701",
            "75404482fcbf4ace948c3b9b0214acf1",
            "2029fa6a12fa44b2893741e352d51b34",
            "020d23bd64b44438983448631063922e",
            "d1b5b2daf3bf4f6a8fdd0a5ebe6715da",
            "b700710fc8af4bda8e228cc2455cb62c",
            "25c3465b4e8c4dc19628bab984b62f36",
            "609884493a1d4aaeb0fc441ad181f92c",
            "51ca89cb2f334e298c1a4699b62e3ed1",
            "3ae11f53ee184654ab03c1457d5419e3",
            "3189acce89f84ef78764cbf9c1c68045",
            "3dce2b33b9924637a9a6fbe783ba811b",
            "5f824b0a376142f3a601f5e5d8a30df6",
            "65f06deb5114492680d3e390da8da5b1",
            "748ce7a50e0f4ff28d576680a6b600c4",
            "7d9f15e3d414464d91d78785fbc10e39",
            "3acdbb39ddd44670b081682d4839efba",
            "141ead2ed08a4891a64d0305474d81c3",
            "94280fd4bfdb41218965b695586bb520",
            "7b770512a5ca446eae2af22785408e17",
            "2f3040af2a614c2dafe853a2c0f1f866",
            "0d7c298ea3474f26ae1ab3dcd58e2def",
            "d0ce6e2890984e17933c3a235d71321b",
            "19e6c80806ee4753ae30ba1535c9cf59",
            "638b9afa00f84081b82e3fe18aee349b",
            "a8c1112528b94b828ce8416b402ce693",
            "f7f5f4cd09aa48e487570f976ac75f05",
            "6d2fba4e535644eda6e1401ff4f98f52",
            "9d5830a4502241b5855e5a9a729a10c4",
            "f8159e6b64fd47df9f7fb63f42c7bc1e",
            "1b69cd9b6cca458e9820c536261f9a2c",
            "81faa901e1024c608696e46958c228b4",
            "815dca6a93054b4cbfb38499d794a79b",
            "6c576f61e2e9469cb007db37901bdd5e",
            "688ec1a24adb4447a0952e46cb8700a3",
            "042d3c6698814858a858c282a3d9504c",
            "addc95dfa8d54d7cb144e56b331284da",
            "cc5f137e3dd0473d80f20d5dd1455fd0",
            "0016757719f74d3fa45ff2e0806e166c",
            "e4002567482e49bd9ebf4d83c796f678",
            "30f433903cdb4accb056df109cc2bc0e",
            "bc2e91bc63794718acca1db9f290ec8c",
            "a5b67b3680b64ef09055a72a70d1924d",
            "8632a846fde84cf4bebd88c9fc2b8a48",
            "4f854465f3834e03940a01e0420010cf",
            "09eeef254f0947eaae3903ca7fdba4de",
            "03f85259f25f463c9e042c0e1ba8e353",
            "da75600e526444318c667216768a4276",
            "91d77046f4404aef9a0d5c18b0acb8e8",
            "3d1fe82970b246bf90d94f89b06932d5",
            "24677428dec54cc18157c71e23b1890d",
            "637d03f4ebd1438fbf864c14079e4d62",
            "d64c3dd103f241cb80d9178a11d87eb2",
            "53cc0e958402425f91761918965d3569",
            "668e0eefd76449d586f7610161d9fb3a",
            "c30437f6fef449d69f6230de407d8641",
            "8804819ffcea43438e91c7cc6122f84d",
            "a03472b8697c4af99f91823a59117655",
            "bdd2053e62324738a5f6f8eda951d21a",
            "dce4f64e14fc4f9c8c0e7acc2e1d7e58",
            "49e1e3a582ad440dbad504adb70b7e51",
            "507afa4a87db4083a86b918ed08455fb",
            "b92cb70afc92434b888ca44b164b5b96",
            "117f1313ef4944f4b0e534d1184a104e",
            "0d2404ecf8cb45dfb062281a67004733",
            "bd70c45b51e8430888ff98bb47a0ea51",
            "1779bae4958346e2a6c99ef0ce06d0ab",
            "2c8a30c3c8bd4557bdb764257380d6d8",
            "1f73c74e610b4031bb66d14defb46bbd",
            "0b4903e0abeb427bbc84fa4d45fc2caa",
            "31974613d0b94f22b643fae177667deb",
            "c653e1aaf93f4cd6ada8ef37443333c3",
            "167a807a17a34e678da028309769651b",
            "9786002a68834670ac15e341ca110dde",
            "992e2f446cf3479a8500bba58c9884d6",
            "fd077a94ab8243e6a71484484432f8cb",
            "97b974494dbd4a80ae35727fe88be98c",
            "bfc3e8b9257a443faeb0df90e12a138f",
            "1213a89aa34d4a58a3a3939f9f2e63e8",
            "1ff7ccbdaa064c76bee7da55cccd6019",
            "75e106271e1145deacf7d0a9e190890b",
            "cf361025eb0447f0886b589048b846d1",
            "28f5b713ca744835a0cc79eefd44b99a",
            "88167240aacc47d290aa8ebed3dd4208",
            "619286e0b6724f419553947f0acc9fc9",
            "055457bc97e74046abe7ee954ca7296d",
            "79ae0d1a5cce4d3f88bd7e294abd4019",
            "9d83af4c2127402b88bfabf06dd70a4f",
            "18c9136964524f6bbb941972277cb889",
            "f6826b924b7e46298f40acc8321d0239",
            "646c95cbe3b74b4fb5db89a7b3aeb594",
            "9faa20402d974e54b81e60639dc0b82a",
            "875b0d69740f498f9fd824609f871af6",
            "85917ca6fbda4bdd9f3a0b38bbeadaa2",
            "fd57202f0a954462a2db70aba25f9004",
            "b3daae12e62442c1abce031cad7f820c",
            "2bb91d3133e34396bb8a5872055b50b4",
            "6b3e7d73311e404f8100d8afce54a3d7",
            "be66714b3f684019a783d12886841b8d",
            "8d3ea40ac98e448ea27a9753fd82ef50",
            "0b5c160e95c04d42bbbd6bccf108b56b",
            "3ef42b0ab7fb43bba47caa743571d247",
            "d3bfa51a5f49436f903b65d65b0e3cc0",
            "55601140b8ec4c1b94a445fb2f5171a3",
            "a316e720eca44ea1b863396bb812c4e5",
            "53347bf0c2554e88bb151b0768abbdb0",
            "3587a9cfc2eb4d21ac5c4b210f548246",
            "ba95dc2d0ca74669bb95d45238a89579",
            "0e8b5481b35e4c2a898800259ba8fdc7",
            "9d915ccf293a41c396e7b34655db9891",
            "3fbdf086c170439d8d1a0629e86dcbdc",
            "729ca9d4251d44c7ad305286ac648452",
            "86f2bc9f6e5047af841999862bf92016",
            "7c310777a2c8441cb78e194df1c10574",
            "f1b327d96b5d4b1797e9d0c87a5a73c0",
            "4a511276fc2b4825beadb2bfb9a2bd38",
            "950d8c781449433fb007a7a8bf727cea",
            "2a411eb897ab43d597f59023ba5013f3",
            "3b2742bf8cca4e64a638de0c2ec2245b",
            "2afaa3040c4e4daba8ec2a98e96857a9",
            "0fdf5cb229a54e1a9916b853b57e5db6",
            "906e849c370e4c4a941046c95a5e19b3",
            "8ce6159ae84a45d294d73cfb5f50d268",
            "a85f7f3c9b2142abbb2488718df072b6",
            "3de9e84c507a4b20889ea8b0ed78d0dc",
            "febd447e4a8f428696852c5fac21becd",
            "92062a9146764f64aec75fc8a3da110f",
            "7ce8fd6dd0df4a80bb078ed382a190c6",
            "4110dc46db0d484cb55916c26df9bc18",
            "96702408efef4c66a418eac2a5460dfe",
            "9133986262044fc2bff5120b59655ce3",
            "081c202c83c24af1a5c686bc579bb1c5",
            "cd683a5d28a749688fb8ef9d41ac7811",
            "26dc7a19cc8e46a3be76d7602b43df50",
            "c1e5b02ea1504d54b1beee7c6144397a",
            "d4e58c81220647b4a36adf849cac4ae7",
            "219b6530825c40708aab9e4b13b33191",
            "3809e653252f4c269c36e939019bc45a"
          ]
        },
        "id": "w9en8hT4kv_H",
        "outputId": "ef99bd15-9eb2-4826-d1b0-41197a08061c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading models...\n",
            "\n",
            "1. liuhaotian/llava-v1.5-7b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44ab6796f4924a99b3549d448a74f2aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fb03ff4604a4b19baa2e5939fa473f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1ec3d3a0b0f4a17aec32de94cf53da7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "489e967075164ed5b761b5ed1f25f805"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d6a3b8678a341f3b4306eadb7728f3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35894742414f4a9a958d53407590cce6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mm_projector.bin:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2e4d9a812fa40dbae4d34207d56bb7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ace3d6f60bd8451c84156fb57a1e0fb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13fcc7efea6644bd9b51341c2a022a8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22b080ec5a94183a0ec33053e00f899"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c38dc9c43e484b0389c5d0ea2d260da5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a8838f013274b3f9cbfcfe87ad82043"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Downloaded to: /root/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234\n",
            "\n",
            "2. openai/clip-vit-large-patch14-336\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25c3465b4e8c4dc19628bab984b62f36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "141ead2ed08a4891a64d0305474d81c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d5830a4502241b5855e5a9a729a10c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4002567482e49bd9ebf4d83c796f678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24677428dec54cc18157c71e23b1890d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tf_model.h5:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "507afa4a87db4083a86b918ed08455fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "167a807a17a34e678da028309769651b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88167240aacc47d290aa8ebed3dd4208"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd57202f0a954462a2db70aba25f9004"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53347bf0c2554e88bb151b0768abbdb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "950d8c781449433fb007a7a8bf727cea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ce8fd6dd0df4a80bb078ed382a190c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Downloaded to: /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "EsbwZIGPKjO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2FnQ8IPL_m4",
        "outputId": "252b7f9f-5564-4e3b-8604-cadef86cbfff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t\t     llava\t\t\t recipeqa_gradcam_3.png  tools\n",
            "checkpoints\t     recipeqa_eval_results.json  results\t\t wandb\n",
            "data\t\t     recipeqa_gradcam_1.png\t scripts\n",
            "gradcam_example.png  recipeqa_gradcam_2.png\t test.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RECIPEQA_DIR = \"../recipeqa\"\n",
        "TRAIN_JSON = os.path.join(RECIPEQA_DIR, \"recipeqa_train.json\")\n",
        "TEST_JSON = os.path.join(RECIPEQA_DIR, \"recipeqa_test.json\")\n",
        "TRAIN_IMAGE_FOLDER = os.path.join(RECIPEQA_DIR, \"train\")\n",
        "TEST_IMAGE_FOLDER = os.path.join(RECIPEQA_DIR, \"test\")\n",
        "\n",
        "# Check files\n",
        "for name, path in [(\"Train JSON\", TRAIN_JSON), (\"TEST JSON\", TEST_JSON)]:\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"{name}: {len(data)} samples\")\n",
        "\n",
        "num_train_images = len([f for f in os.listdir(TRAIN_IMAGE_FOLDER) if f.endswith('.jpg')])\n",
        "num_test_images = len([f for f in os.listdir(TEST_IMAGE_FOLDER) if f.endswith('.jpg')])\n",
        "print(f\"Train Images: {num_train_images}\")\n",
        "print(f\"TEST Images: {num_test_images}\")\n",
        "\n",
        "# Show sample\n",
        "with open(TRAIN_JSON) as f:\n",
        "    print(\"\\nExample training sample:\")\n",
        "    print(json.dumps(json.load(f)[0], indent=2))"
      ],
      "metadata": {
        "id": "XQTfjbcZlBux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcb3ee5-ddb6-42f7-957f-57273d09f942"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train JSON: 200 samples\n",
            "TEST JSON: 80 samples\n",
            "Train Images: 200\n",
            "TEST Images: 80\n",
            "\n",
            "Example training sample:\n",
            "{\n",
            "  \"id\": \"recipeqa_1_0\",\n",
            "  \"image\": \"1_0_false.jpg\",\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"Identify if from left to right is the correct order of cooking this how to cook fava beans. Answer only 'true' or 'false'.\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"false\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure each human prompt explicitly includes an <image> token for multimodal training\n",
        "import json, os\n",
        "from llava.constants import DEFAULT_IMAGE_TOKEN\n",
        "\n",
        "def add_image_token(path):\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    updated = 0\n",
        "    for sample in data:\n",
        "        conv = sample.get(\"conversations\", [])\n",
        "        if conv and conv[0].get(\"from\") == \"human\":\n",
        "            text = conv[0].get(\"value\", \"\")\n",
        "            if DEFAULT_IMAGE_TOKEN not in text:\n",
        "                conv[0][\"value\"] = f\"{DEFAULT_IMAGE_TOKEN}\\n{text}\"\n",
        "                updated += 1\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    print(f\"{os.path.basename(path)}: added <image> to {updated} samples\")\n",
        "\n",
        "for path in [TRAIN_JSON, TEST_JSON]:\n",
        "    add_image_token(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7i3G5EAFEP8",
        "outputId": "074bcc2a-e9e9-4e6c-aeae-159f0a9f4234"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recipeqa_train.json: added <image> to 200 samples\n",
            "recipeqa_test.json: added <image> to 80 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configs"
      ],
      "metadata": {
        "id": "SNPlSCXgKu1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"./checkpoints/viscot_recipe_prefix\"\n",
        "\n",
        "### experiment 1\n",
        "NUM_EPOCHS = 2\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 8\n",
        "# LR = 1e-3\n",
        "\n",
        "### experiment 2\n",
        "# NUM_EPOCHS = 3\n",
        "LR = 2e-4\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE} x {GRAD_ACCUM} = {BATCH_SIZE*GRAD_ACCUM}\")\n",
        "print(f\"Learning Rate: {LR}\")"
      ],
      "metadata": {
        "id": "EGoIC7EUm0sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60295f4-bf02-4032-a143-4997ed22a39f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: ./checkpoints/viscot_recipe_prefix\n",
            "Epochs: 2\n",
            "Batch Size: 2 x 8 = 16\n",
            "Learning Rate: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set arguments\n",
        "sys.argv = [\n",
        "    'train',\n",
        "    '--model_name_or_path', LLAVA_MODEL,\n",
        "    '--version', 'v1',\n",
        "    '--vision_tower', VISION_TOWER,\n",
        "    '--mm_projector_type', 'mlp2x_gelu',\n",
        "    '--mm_vision_select_layer', '-2',\n",
        "    '--mm_use_im_start_end', 'False',\n",
        "    '--mm_use_im_patch_token', 'False',\n",
        "\n",
        "    # Prefix tuning: freeze backbone, train projector only\n",
        "    '--freeze_backbone', 'True',\n",
        "    '--tune_mm_mlp_adapter', 'True',\n",
        "    '--freeze_mm_mlp_adapter', 'False',\n",
        "\n",
        "    # Data\n",
        "    '--data_path', TRAIN_JSON,\n",
        "    '--is_multimodal', 'True',\n",
        "    '--image_folder', TRAIN_IMAGE_FOLDER,\n",
        "    '--image_aspect_ratio', 'pad',\n",
        "    '--group_by_modality_length', 'False',\n",
        "    '--lazy_preprocess', 'True',\n",
        "\n",
        "    # Training parameters (use variables from cell-15)\n",
        "    '--bf16', 'False',\n",
        "    '--fp16', 'True',\n",
        "    '--output_dir', OUTPUT_DIR,\n",
        "    '--num_train_epochs', str(NUM_EPOCHS),\n",
        "    '--per_device_train_batch_size', str(BATCH_SIZE),\n",
        "    '--gradient_accumulation_steps', str(GRAD_ACCUM),\n",
        "    '--evaluation_strategy', 'no',\n",
        "    '--save_strategy', 'epoch',\n",
        "    '--save_total_limit', '3',\n",
        "    '--learning_rate', str(LR),\n",
        "    '--weight_decay', '0.0',\n",
        "    '--warmup_ratio', '0.03',\n",
        "    '--lr_scheduler_type', 'cosine',\n",
        "    '--logging_steps', '10',\n",
        "    '--tf32', 'False',\n",
        "    '--model_max_length', '2048',\n",
        "    '--gradient_checkpointing', 'True',\n",
        "    '--dataloader_num_workers', '4',\n",
        "    # '--report_to', 'wandb',\n",
        "    '--report_to', 'tensorboard',\n",
        "]\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"Method: Prefix tuning (train projector only, freeze backbone)\\n\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE} x {GRAD_ACCUM} = {BATCH_SIZE*GRAD_ACCUM}\")\n",
        "print(f\"Total epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LR}\\n\")\n",
        "\n",
        "try:\n",
        "    train()\n",
        "    print(\"\\nTraining completed!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "2ba162d9b0354f80918f056770266f37",
            "5ddb234a215e4ce1b0f4c0a8b3ea46b9",
            "bfff1aa382cc4bd58f66557db1990a78",
            "a85f696911b447188ab25d0621c7cc8c",
            "ea89814c5bdb462c9eb0ea7dd46db294",
            "4c2dcf4450044d7fb614a56bcdf607ae",
            "bfd07fed23df477b8df58e4e4349a049",
            "e35bc9985e2f4829806eeec6316e01d1",
            "cdbe4c15c3f34c85961e36cad6bbaeac",
            "2cf01b2e6e7c402fb4af340dd74e84fd",
            "b550b70d4aaa4bd69f035b2cf521945f"
          ]
        },
        "id": "Wf6m8i4KE_FS",
        "outputId": "3fafdaca-5de6-4396-89c0-efeb83860d6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type llava to instantiate a model of type viscot_llava. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Method: Prefix tuning (train projector only, freeze backbone)\n",
            "\n",
            "Effective batch size: 2 x 8 = 16\n",
            "Total epochs: 2\n",
            "Learning rate: 0.0002\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ba162d9b0354f80918f056770266f37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting inputs...Skip in lazy mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 01:41, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.856900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.367300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking"
      ],
      "metadata": {
        "id": "U6KZRRovMpCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "t4h34-loK19F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import get_model_name_from_path\n",
        "import torch\n",
        "\n",
        "checkpoints = sorted([d for d in os.listdir(OUTPUT_DIR) if d.startswith('checkpoint-')])\n",
        "\n",
        "if checkpoints:\n",
        "    checkpoint_path = os.path.join(OUTPUT_DIR, checkpoints[-1])\n",
        "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    print(\"Loading base LLaVA model...\")\n",
        "    print(f\"Model path: {LLAVA_MODEL}\")\n",
        "    print(f\"Vision tower: {VISION_TOWER}\")\n",
        "\n",
        "    # Load base model\n",
        "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "        model_path=LLAVA_MODEL,\n",
        "        model_base=None,\n",
        "        model_name=get_model_name_from_path(LLAVA_MODEL),\n",
        "        load_8bit=False,\n",
        "        load_4bit=False,\n",
        "        # torch_dtype=torch.float16,\n",
        "        # low_cpu_mem_usage=False,\n",
        "        device_map={\"\": \"cuda:0\"},\n",
        "    )\n",
        "    model = model.cuda().half()\n",
        "\n",
        "\n",
        "    print(\"\\nInitializing vision tower...\")\n",
        "    vision_tower = model.get_vision_tower()\n",
        "    if not vision_tower.is_loaded:\n",
        "        vision_tower.load_model()\n",
        "\n",
        "    print(\"Moving model to GPU and converting to FP16...\")\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Convert ALL components to FP16\n",
        "    model = model.half()  # Convert entire model to FP16\n",
        "\n",
        "    print(\"✓ Converted to FP16 successfully\")\n",
        "\n",
        "    # Test vision tower\n",
        "    print(\"\\nTesting vision tower...\")\n",
        "    dummy_img = torch.randn(1, 3, 336, 336).half().cuda()\n",
        "    with torch.no_grad():\n",
        "        test_features = vision_tower(dummy_img)\n",
        "\n",
        "    print(f\"Vision output shape: {test_features.shape}\")\n",
        "    print(f\"Has NaN: {torch.isnan(test_features).any().item()}\")\n",
        "    print(f\"Has Inf: {torch.isinf(test_features).any().item()}\")\n",
        "\n",
        "    if torch.isnan(test_features).any():\n",
        "        print(\"\\n ERROR: Vision tower still produces NaN!\")\n",
        "        print(\"Trying to reload vision tower from scratch...\")\n",
        "        from transformers import CLIPVisionModel\n",
        "        vision_model = CLIPVisionModel.from_pretrained(VISION_TOWER).cuda().half()\n",
        "        model.get_vision_tower().vision_tower = vision_model\n",
        "\n",
        "        # Test again\n",
        "        with torch.no_grad():\n",
        "            test_features = vision_tower(dummy_img)\n",
        "        print(f\"After reload - Has NaN: {torch.isnan(test_features).any().item()}\")\n",
        "\n",
        "    # Load fine-tuned projector weights\n",
        "    mm_projector_path = os.path.join(checkpoint_path, 'mm_projector.bin')\n",
        "    print(f\"\\nLoading fine-tuned projector from: {mm_projector_path}\")\n",
        "\n",
        "    projector_weights = torch.load(mm_projector_path, map_location='cpu')\n",
        "\n",
        "    # Convert projector weights to FP16 and move to GPU\n",
        "    for key in projector_weights.keys():\n",
        "        projector_weights[key] = projector_weights[key].half().cuda()\n",
        "\n",
        "    # Load the weights\n",
        "    msg = model.load_state_dict(projector_weights, strict=False)\n",
        "    print(f\"Loaded projector - Missing: {len(msg.missing_keys)}, Unexpected: {len(msg.unexpected_keys)}\")\n",
        "\n",
        "    # Final sanity check\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Final model state:\")\n",
        "    print(f\"  Device: {next(model.parameters()).device}\")\n",
        "    print(f\"  Dtype: {next(model.parameters()).dtype}\")\n",
        "\n",
        "    # Test full pipeline\n",
        "    print(\"\\nFinal vision tower test:\")\n",
        "    with torch.no_grad():\n",
        "        vision_out = vision_tower(dummy_img)\n",
        "        projector_out = model.get_model().mm_projector(vision_out)\n",
        "\n",
        "    print(f\"  Vision output has NaN: {torch.isnan(vision_out).any().item()}\")\n",
        "    print(f\"  Projector output has NaN: {torch.isnan(projector_out).any().item()}\")\n",
        "    print(f\"  Vision output mean: {vision_out.mean().item()}\")\n",
        "\n",
        "    if not torch.isnan(projector_out).any():\n",
        "        print(\"\\n✓ Model loaded successfully without NaN!\")\n",
        "    else:\n",
        "        print(\"\\n❌ WARNING: Still have NaN in the pipeline!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "else:\n",
        "    print(\"No checkpoints found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712,
          "referenced_widgets": [
            "6d27f9cee71e4877a47636882f72b4a7",
            "86ad5cbf0ee04fb7a3379c58aeec7060",
            "ec394db6bfe44050aa61df03817c6a9d",
            "2f715b42a5b5467a9df4ff357cb63e1e",
            "e4f809c02c884d1aa3f1580e6f08ae8d",
            "383e6ec1a53647b493531c07b342bcf6",
            "d1974425d68649a09fc9bb00979c8b1e",
            "d06a640c3ec94a31914f9e26489b7df5",
            "f9b80793b5764b6bafacb74db88ad3b8",
            "c865edba8eee4ba1883f82f366bdf29c",
            "205f01fca2db445da85e011ce7060a55"
          ]
        },
        "id": "dtaAzIcwF8y6",
        "outputId": "2dfe9f45-eaaa-4006-bd3b-f0cd0baa79dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type llava to instantiate a model of type viscot_llava. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint: ./checkpoints/viscot_recipe_prefix/checkpoint-24\n",
            "Loading base LLaVA model...\n",
            "Model path: liuhaotian/llava-v1.5-7b\n",
            "Vision tower: openai/clip-vit-large-patch14-336\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d27f9cee71e4877a47636882f72b4a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing vision tower...\n",
            "Moving model to GPU and converting to FP16...\n",
            "✓ Converted to FP16 successfully\n",
            "\n",
            "Testing vision tower...\n",
            "Vision output shape: torch.Size([1, 576, 1024])\n",
            "Has NaN: True\n",
            "Has Inf: False\n",
            "\n",
            " ERROR: Vision tower still produces NaN!\n",
            "Trying to reload vision tower from scratch...\n",
            "After reload - Has NaN: False\n",
            "\n",
            "Loading fine-tuned projector from: ./checkpoints/viscot_recipe_prefix/checkpoint-24/mm_projector.bin\n",
            "Loaded projector - Missing: 714, Unexpected: 0\n",
            "\n",
            "============================================================\n",
            "Final model state:\n",
            "  Device: cuda:0\n",
            "  Dtype: torch.float16\n",
            "\n",
            "Final vision tower test:\n",
            "  Vision output has NaN: False\n",
            "  Projector output has NaN: False\n",
            "  Vision output mean: 0.05450439453125\n",
            "\n",
            "✓ Model loaded successfully without NaN!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from llava.mm_utils import tokenizer_image_token\n",
        "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
        "\n",
        "\n",
        "def _safe_div(num: float, den: float) -> float:\n",
        "    return num / den if den else 0.0\n",
        "\n",
        "\n",
        "def evaluate(model, tokenizer, image_processor, test_json, image_folder):\n",
        "    \"\"\"Evaluate model on a true/false task with full precision/recall/F1 metrics.\"\"\"\n",
        "    with open(test_json, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    results = []\n",
        "    unknown = 0\n",
        "    model.eval()\n",
        "\n",
        "    for i, sample in enumerate(data):\n",
        "        img_path = os.path.join(image_folder, sample['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Ensure the prompt starts with an <image> token\n",
        "        prompt = sample['conversations'][0]['value']\n",
        "        if DEFAULT_IMAGE_TOKEN not in prompt:\n",
        "            prompt = f\"{DEFAULT_IMAGE_TOKEN}{prompt}\"\n",
        "\n",
        "        ground_truth = sample['conversations'][1]['value'].strip().lower()\n",
        "        # Expected labels: \"true\" or \"false\"\n",
        "\n",
        "        # Process image\n",
        "        img_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\n",
        "\n",
        "        # Tokenize prompt\n",
        "        input_ids = tokenizer_image_token(\n",
        "            prompt,\n",
        "            tokenizer,\n",
        "            IMAGE_TOKEN_INDEX,\n",
        "            return_tensors='pt'\n",
        "        ).unsqueeze(0).cuda()\n",
        "\n",
        "        # Generate model output\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(\n",
        "                input_ids,\n",
        "                images=img_tensor,\n",
        "                do_sample=False,\n",
        "                max_new_tokens=128,\n",
        "                use_cache=True,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode output\n",
        "        generated_ids = out[0, input_ids.shape[1]:].tolist()\n",
        "\n",
        "        valid_ids = []\n",
        "        for token_id in generated_ids:\n",
        "            if token_id == IMAGE_TOKEN_INDEX or token_id < 0 or token_id >= vocab_size:\n",
        "                continue\n",
        "            if token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "            valid_ids.append(token_id)\n",
        "\n",
        "        output = tokenizer.decode(valid_ids, skip_special_tokens=True).strip()\n",
        "        output_lower = output.lower()\n",
        "\n",
        "        # ===== True/False Classification =====\n",
        "        if \"true\" in output_lower:\n",
        "            predicted = \"true\"\n",
        "        elif \"false\" in output_lower:\n",
        "            predicted = \"false\"\n",
        "        else:\n",
        "            predicted = \"unknown\"\n",
        "\n",
        "        is_correct = predicted == ground_truth\n",
        "\n",
        "        results.append({\n",
        "            'id': sample['id'],\n",
        "            'image': sample['image'],\n",
        "            'ground_truth': ground_truth,\n",
        "            'predicted': predicted,\n",
        "            'full_output': output,\n",
        "            'correct': is_correct\n",
        "        })\n",
        "\n",
        "        if predicted == 'unknown':\n",
        "            unknown += 1\n",
        "\n",
        "        # --- Print progress ---\n",
        "        print(f\"[{i+1}/{len(data)}] {sample['id']}\")\n",
        "        print(f\"  GT: {ground_truth}\")\n",
        "        print(f\"  Pred: {predicted}\")\n",
        "        print(f\"  Full output: {output[:200]}\")\n",
        "        print(\"  ✓ Correct\" if is_correct else \"  ✗ Wrong\")\n",
        "\n",
        "    # ======================================================\n",
        "    # Compute metrics\n",
        "    # ======================================================\n",
        "\n",
        "    labels = [\"true\", \"false\"]\n",
        "    conf = {c: {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"count\": 0} for c in labels}\n",
        "\n",
        "    for r in results:\n",
        "        gt = r[\"ground_truth\"]\n",
        "        pred = r[\"predicted\"]\n",
        "\n",
        "        if gt in labels:\n",
        "            conf[gt][\"count\"] += 1\n",
        "\n",
        "        if pred not in labels:\n",
        "            if gt in labels:\n",
        "                conf[gt][\"fn\"] += 1\n",
        "            continue\n",
        "\n",
        "        for c in labels:\n",
        "            if pred == c and gt == c:\n",
        "                conf[c][\"tp\"] += 1\n",
        "            elif pred == c and gt != c:\n",
        "                conf[c][\"fp\"] += 1\n",
        "            elif gt == c and pred != c:\n",
        "                conf[c][\"fn\"] += 1\n",
        "\n",
        "    metrics = {}\n",
        "    for c in labels:\n",
        "        tp, fp, fn = conf[c][\"tp\"], conf[c][\"fp\"], conf[c][\"fn\"]\n",
        "        prec = _safe_div(tp, tp + fp)\n",
        "        rec = _safe_div(tp, tp + fn)\n",
        "        f1 = _safe_div(2 * prec * rec, prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "        metrics[f\"{c}_precision\"] = prec\n",
        "        metrics[f\"{c}_recall\"] = rec\n",
        "        metrics[f\"{c}_f1\"] = f1\n",
        "\n",
        "    # Macro average\n",
        "    metrics[\"macro_precision\"] = (metrics[\"true_precision\"] + metrics[\"false_precision\"]) / 2\n",
        "    metrics[\"macro_recall\"] = (metrics[\"true_recall\"] + metrics[\"false_recall\"]) / 2\n",
        "    metrics[\"macro_f1\"] = (metrics[\"true_f1\"] + metrics[\"false_f1\"]) / 2\n",
        "\n",
        "    total_count = conf[\"true\"][\"count\"] + conf[\"false\"][\"count\"]\n",
        "\n",
        "    # Weighted metrics\n",
        "    metrics[\"weighted_precision\"] = (\n",
        "        (conf[\"true\"][\"count\"] * metrics[\"true_precision\"] +\n",
        "         conf[\"false\"][\"count\"] * metrics[\"false_precision\"]) / total_count\n",
        "    )\n",
        "\n",
        "    metrics[\"weighted_recall\"] = (\n",
        "        (conf[\"true\"][\"count\"] * metrics[\"true_recall\"] +\n",
        "         conf[\"false\"][\"count\"] * metrics[\"false_recall\"]) / total_count\n",
        "    )\n",
        "\n",
        "    metrics[\"weighted_f1\"] = (\n",
        "        (conf[\"true\"][\"count\"] * metrics[\"true_f1\"] +\n",
        "         conf[\"false\"][\"count\"] * metrics[\"false_f1\"]) / total_count\n",
        "    )\n",
        "\n",
        "    # Accuracy and unknown ratio\n",
        "    metrics[\"accuracy\"] = sum(1 for r in results if r[\"correct\"]) / len(results)\n",
        "    metrics[\"unknown_frac\"] = _safe_div(unknown, len(results))\n",
        "\n",
        "    # ======================================================\n",
        "    # Print Summary\n",
        "    # ======================================================\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\n",
        "        f\"Accuracy: {metrics['accuracy']:.4f} | \"\n",
        "        f\"Weighted P/R/F1: {metrics['weighted_precision']:.4f} / \"\n",
        "        f\"{metrics['weighted_recall']:.4f} / \"\n",
        "        f\"{metrics['weighted_f1']:.4f} | \"\n",
        "        f\"Unknown: {metrics['unknown_frac']:.4f}\"\n",
        "    )\n",
        "\n",
        "    for c in labels:\n",
        "        print(\n",
        "            f\"{c.capitalize():>6} - \"\n",
        "            f\"P: {metrics[f'{c}_precision']:.4f} \"\n",
        "            f\"R: {metrics[f'{c}_recall']:.4f} \"\n",
        "            f\"F1: {metrics[f'{c}_f1']:.4f} \"\n",
        "            f\"(count={conf[c]['count']})\"\n",
        "        )\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return results, metrics"
      ],
      "metadata": {
        "id": "UAs9w9YCF9Xl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run evaluation\n",
        "results, metrics = evaluate(model, tokenizer, image_processor, TEST_JSON, TEST_IMAGE_FOLDER)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guipu-unF9Vc",
        "outputId": "c22f9300-4743-46ff-ee04-50f010a48493"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/80] recipeqa_51_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[2/80] recipeqa_51_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[3/80] recipeqa_51_2\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[4/80] recipeqa_51_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[5/80] recipeqa_52_0\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[6/80] recipeqa_52_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[7/80] recipeqa_52_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[8/80] recipeqa_52_3\n",
            "  GT: true\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[9/80] recipeqa_53_0\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[10/80] recipeqa_53_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[11/80] recipeqa_53_2\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[12/80] recipeqa_53_3\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[13/80] recipeqa_54_0\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[14/80] recipeqa_54_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[15/80] recipeqa_54_2\n",
            "  GT: true\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[16/80] recipeqa_54_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[17/80] recipeqa_55_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[18/80] recipeqa_55_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[19/80] recipeqa_55_2\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[20/80] recipeqa_55_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[21/80] recipeqa_56_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[22/80] recipeqa_56_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[23/80] recipeqa_56_2\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[24/80] recipeqa_56_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[25/80] recipeqa_57_0\n",
            "  GT: false\n",
            "  Pred: true\n",
            "  Full output: true\n",
            "  ✗ Wrong\n",
            "[26/80] recipeqa_57_1\n",
            "  GT: false\n",
            "  Pred: true\n",
            "  Full output: true\n",
            "  ✗ Wrong\n",
            "[27/80] recipeqa_57_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[28/80] recipeqa_57_3\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[29/80] recipeqa_58_0\n",
            "  GT: true\n",
            "  Pred: true\n",
            "  Full output: true\n",
            "  ✓ Correct\n",
            "[30/80] recipeqa_58_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[31/80] recipeqa_58_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[32/80] recipeqa_58_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[33/80] recipeqa_59_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[34/80] recipeqa_59_1\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[35/80] recipeqa_59_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[36/80] recipeqa_59_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[37/80] recipeqa_60_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[38/80] recipeqa_60_1\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[39/80] recipeqa_60_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[40/80] recipeqa_60_3\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[41/80] recipeqa_61_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[42/80] recipeqa_61_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[43/80] recipeqa_61_2\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[44/80] recipeqa_61_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[45/80] recipeqa_62_0\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[46/80] recipeqa_62_1\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[47/80] recipeqa_62_2\n",
            "  GT: false\n",
            "  Pred: true\n",
            "  Full output: true\n",
            "  ✗ Wrong\n",
            "[48/80] recipeqa_62_3\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[49/80] recipeqa_63_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[50/80] recipeqa_63_1\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[51/80] recipeqa_63_2\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[52/80] recipeqa_63_3\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[53/80] recipeqa_64_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[54/80] recipeqa_64_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[55/80] recipeqa_64_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[56/80] recipeqa_64_3\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[57/80] recipeqa_65_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[58/80] recipeqa_65_1\n",
            "  GT: true\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[59/80] recipeqa_65_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[60/80] recipeqa_65_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[61/80] recipeqa_66_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[62/80] recipeqa_66_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[63/80] recipeqa_66_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[64/80] recipeqa_66_3\n",
            "  GT: true\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[65/80] recipeqa_67_0\n",
            "  GT: true\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[66/80] recipeqa_67_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[67/80] recipeqa_67_2\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[68/80] recipeqa_67_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[69/80] recipeqa_68_0\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[70/80] recipeqa_68_1\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[71/80] recipeqa_68_2\n",
            "  GT: true\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[72/80] recipeqa_68_3\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[73/80] recipeqa_69_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[74/80] recipeqa_69_1\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[75/80] recipeqa_69_2\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[76/80] recipeqa_69_3\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[77/80] recipeqa_70_0\n",
            "  GT: false\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✓ Correct\n",
            "[78/80] recipeqa_70_1\n",
            "  GT: true\n",
            "  Pred: false\n",
            "  Full output: false\n",
            "  ✗ Wrong\n",
            "[79/80] recipeqa_70_2\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "[80/80] recipeqa_70_3\n",
            "  GT: false\n",
            "  Pred: unknown\n",
            "  Full output: \n",
            "  ✗ Wrong\n",
            "\n",
            "============================================================\n",
            "Accuracy: 0.5500 | Weighted P/R/F1: 0.6384 / 0.5500 / 0.5769 | Unknown: 0.2500\n",
            "  True - P: 0.2500 R: 0.0500 F1: 0.0833 (count=20)\n",
            " False - P: 0.7679 R: 0.7167 F1: 0.7414 (count=60)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ku6BSuF1F9TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iy-rOT4QF9RE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}